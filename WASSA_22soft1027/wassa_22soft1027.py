# -*- coding: utf-8 -*-
"""WASSA_22soft1027.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WLtd7bWn0WU5jH7NOhz-BnVBEsr6gcMe

# **3. Go Abdul-Mageed & Ungar (2017) - WASSA Emotion Intensity Dataset**
"""

# Imports

# NLP utilities
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
# Core libraries
import numpy as np
import pandas as pd
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
# Sentiment analysis
from textblob import TextBlob
# Feature extraction
from sklearn.feature_extraction.text import CountVectorizer
# File handling
from google.colab import files
import glob
# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

#  Uploading Files
print(" Please upload all your .txt dataset files (train/dev/test for all emotions)")
uploaded = files.upload()

#loading datasets

def load_emotion_dataset():
    """
    Loads all emotion .txt files in the directory and combines them
    into a single DataFrame.
    """
    dfs = []

    for filepath in glob.glob("*.txt"):
        # Extracting emotion from filename
        emotion = filepath.split('-')[0]

        # Reading file (third column ignored)
        df = pd.read_csv(
            filepath,
            sep='\t',
            header=None,
            names=['tweet', 'file_emotion', 'intensity'],
            quoting=3,
            engine='python',
            on_bad_lines='skip'
        )

        # Assigning correct emotion based on filename
        df['emotion'] = emotion

        # Keeping only tweet & emotion
        dfs.append(df[['tweet', 'emotion']])
    # Merging all emotions into one DataFrame
    return pd.concat(dfs, ignore_index=True)

df = load_emotion_dataset()

print(df.head())
print(df.tail())
print(df.columns)

# Removing empty tweets
df.dropna(subset=['tweet', 'emotion'], inplace=True)

# Converting tweets to string
df['tweet'] = df['tweet'].astype(str)

# checking
print(df.isnull().sum())

#Preprocessing
def preprocess(text_series):
    """
    Performs standard tweet cleaning while preserving emojis
    """

    # Lowercasing
    text_series = text_series.str.lower()

    # removing HTML
    text_series = text_series.str.replace("(<br/>)", "", regex=True)
    text_series = text_series.str.replace('(<a).*(>).*(</a>)', '', regex=True)

    # replacing HTML encoded symbols
    text_series = text_series.str.replace('(&amp)', '&', regex=True)
    text_series = text_series.str.replace('(&gt)', '>', regex=True)
    text_series = text_series.str.replace('(&lt)', '<', regex=True)
    text_series = text_series.str.replace('(\xa0)', ' ', regex=True)

    # removing URLs, mentions, hashtags
    text_series = text_series.str.replace(r"http\S+|www.\S+", "", regex=True)
    text_series = text_series.str.replace(r"@\w+", "", regex=True)
    text_series = text_series.str.replace(r"#", "", regex=True)

    # keeping emojis
    text_series = text_series.str.replace(r"[^\w\s" + "\U0001F600-\U0001F64F" + "]", "", regex=True)

    text_series = text_series.str.strip()

    return text_series

df['tweet'] = preprocess(df['tweet'])

# Feature Engineering
# Sentiment-based features
df['polarity'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['subjectivity'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

# Length-based features
df['word_count'] = df['tweet'].apply(lambda x: len(str(x).split()))
df['char_count'] = df['tweet'].apply(lambda x: len(str(x)))
df['word_count'] = df['tweet'].apply(lambda x: len(str(x).split()))
df['char_count'] = df['tweet'].apply(lambda x: len(str(x)))

print("Descriptive Statistics:")
print(df[['word_count', 'char_count']].describe())

# Word Count Distribution & Character Count Distribution
features = ['word_count', 'char_count']
titles = ['Word Count Distribution', 'Character Count Distribution']
colors = ['#ffcc99', '#c2c2f0']

for feature, title, color in zip(features, titles, colors):
    sns.histplot(df[feature], bins=40, color=color, kde=True)
    plt.title(title, fontsize=14)
    plt.xlabel(feature)
    plt.ylabel("Frequency")
    plt.show()

# Emotion label distribution
plt.figure(figsize=(8,5))
sns.countplot(x='emotion', data=df, palette='viridis')
plt.title('Emotion Distribution', fontsize=15)
plt.show()

# Polarity by Emotion
plt.figure(figsize=(10,6))
sns.boxplot(x='emotion', y='polarity', data=df, palette='viridis')
plt.title('Sentiment Polarity by Emotion', size=15)
plt.show()

# Word Frequency (N-grams)
def get_top_ngrams(corpus, ngram_range=(1,1), stop_words='english', n=20):
    """
    Extracts top-N most frequent n-grams from corpus.
    """
    vec = CountVectorizer(stop_words=stop_words, ngram_range=ngram_range).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    df_words = pd.DataFrame(words_freq[:n], columns=['Word', 'Freq'])
    return df_words

stop_words = 'english'
n = 20
unigrams = get_top_ngrams(df['tweet'], (1, 1), stop_words, n)
bigrams = get_top_ngrams(df['tweet'], (2, 2), stop_words, n)
trigrams = get_top_ngrams(df['tweet'], (3, 3), stop_words, n)

plt.figure(figsize=(8, 10))
sns.barplot(x='Freq', y='Word', data=unigrams, color='#66b3ff')
plt.title('Top 20 Unigrams (after stopword removal)', size=15)
plt.show()

plt.figure(figsize=(8, 10))
sns.barplot(x='Freq', y='Word', data=bigrams, color='#ff6666')
plt.title('Top 20 Bigrams (after stopword removal)', size=15)
plt.show()

plt.figure(figsize=(8, 10))
sns.barplot(x='Freq', y='Word', data=trigrams, color='#99ff99')
plt.title('Top 20 Trigrams (after stopword removal)', size=15)
plt.show()

df['emotion'].value_counts()

# Part of Speech Tagging of Tweets
import nltk

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

all_tags = []
for tweet in df['tweet']:
    if tweet.strip():
        tokens = nltk.word_tokenize(tweet)
        tags = nltk.pos_tag(tokens)
        all_tags.extend(tags)

import pandas as pd
pos_df = pd.DataFrame(all_tags, columns=['word', 'pos'])
top_pos = pos_df['pos'].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 8))
sns.barplot(y=top_pos.index, x=top_pos.values, color='#ffcc99')
plt.title('Part of Speech Tagging of Tweets', size=15)
plt.show()

# Training & test split
from sklearn.model_selection import train_test_split

X = df['tweet']
y = df['emotion']

# split preserves emotion proportions
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Training size:", len(X_train))
print("Test size:", len(X_test))

df['emotion'].value_counts(normalize=True)

# Baseline
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score

# Majority-class baseline
baseline_pipeline = Pipeline([
    ("tfidf", TfidfVectorizer()), # Text into TF-IDF features
    ("clf", DummyClassifier(strategy='most_frequent')) # "most_frequent" = always predict the most common emotion
])

# Fitting baseline on training data
baseline_pipeline.fit(X_train, y_train)

# Predicting on test data
pred = baseline_pipeline.predict(X_test)

# baseline performance summary
print(" BASELINE PIPELINE ")
print("Accuracy:", accuracy_score(y_test, pred))
print("F1_macro:", f1_score(y_test, pred, average='macro'))
print("\nClassification Report:\n", classification_report(y_test, pred))

# CLASSIFIER COMPARISON
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

from sklearn.pipeline import Pipeline

models = {
    "LogisticRegression": LogisticRegression(max_iter=2000),
    "LinearSVC": LinearSVC()
}

print("CLASSIFIER COMPARISON")

# Train and evaluate each classifier
for name, model in models.items():
    print(f"\nTraining {name}...\n")

    pipe = Pipeline([
        ("tfidf", TfidfVectorizer()),
        ("clf", model)
    ])
    # Fitting the model on the training data
    pipe.fit(X_train, y_train)
    # Predicting emotions on the test data
    preds = pipe.predict(X_test)

    # Displaying precision, recall, and F1-score for each emotion
    print(f"\n{name} Classification Report:")
    print(classification_report(y_test, preds))

#  HYPERPARAMETER OPTIMIZATION (LogReg + LinearSVC)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

print(" HYPERPARAMETER OPTIMIZATION")

# Defining Pipelines

pipe_logreg = Pipeline([
    ("tfidf", TfidfVectorizer()), # Text to TF-IDF features
    ("clf", LogisticRegression(max_iter=3000)) # Logistic Regression classifier
])

pipe_svm = Pipeline([
    ("tfidf", TfidfVectorizer()), # Text to TF-IDF features
    ("clf", LinearSVC()) # Linear Support Vector Classifier
])

# Hyperparameter Search Spaces

# TF-IDF parameters:
# - min_df: ignore rare words appearing in fewer documents
# - ngram_range: use unigrams or unigrams + bigrams
# - sublinear_tf: apply log-scaling to term frequencies

# Classifier parameters:
# - C: regularization strength (smaller = stronger regularization)

logreg_params = {
    "tfidf__min_df": [1, 2, 3, 5],
    "tfidf__ngram_range": [(1,1), (1,2)],
    "tfidf__sublinear_tf": [True, False],
    # Logistic Regression specific
    "clf__C": uniform(0.1, 10),
    "clf__penalty": ["l2"], # L2 regularization
    "clf__solver": ["lbfgs", "liblinear"], # Solvers supporting L2
}

svm_params = {
    "tfidf__min_df": [1, 2, 3, 5],
    "tfidf__ngram_range": [(1,1), (1,2)],
    "tfidf__sublinear_tf": [True, False],
    # Linear SVC specific
    "clf__C": uniform(0.1, 10)
}

# Running RandomizedSearchCV

logreg_search = RandomizedSearchCV(
    pipe_logreg,
    logreg_params,
    n_iter=20, # Number of random configurations
    scoring="f1_macro", # Evaluation metric
    cv=3, # Cross-validation folds
    verbose=2, # Show training progress
    random_state=42,
    n_jobs=-1
)

svm_search = RandomizedSearchCV(
    pipe_svm,
    svm_params,
    n_iter=20,
    scoring="f1_macro",
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

print("\nRunning HPO for LogisticRegression...\n")
logreg_search.fit(X_train, y_train)

print("\nRunning HPO for LinearSVC...\n")
svm_search.fit(X_train, y_train)

# Evaluating best models

best_logreg = logreg_search.best_estimator_
best_svm = svm_search.best_estimator_

# Logistic Regression results
print("BEST LOGISTIC REGRESSION MODEL ")
print(logreg_search.best_params_)
pred_logreg = best_logreg.predict(X_test)
print(classification_report(y_test, pred_logreg))

# Linear SVC results
print(" BEST LINEAR SVC MODEL ")
print(svm_search.best_params_)
pred_svm = best_svm.predict(X_test)
print(classification_report(y_test, pred_svm))

# Store HPO models for further analysis
all_model_pipelines = {
    "HPO_LogisticRegression": best_logreg,
    "HPO_LinearSVC": best_svm
}

print("\nHPO COMPLETE. Models stored in all_model_pipelines.\n")

all_model_pipelines = {
    "Baseline Dummy": baseline_pipeline,
    "HPO LogisticRegression": best_logreg,
    "HPO LinearSVC": best_svm
}

all_predictions = {name: model.predict(X_test) for name, model in all_model_pipelines.items()}

# ADVANCED ERROR ANALYSIS

# imports
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, f1_score
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter

# Recreating and refiting the baseline pipeline
# Ensuring baseline is wrapped in TF-IDF pipeline
baseline_pipeline = Pipeline([
    ("tfidf", TfidfVectorizer()),
    ("clf", DummyClassifier(strategy='most_frequent'))
])
baseline_pipeline.fit(X_train, y_train)

# Collecting all pipelines
all_model_pipelines = {
    "Baseline Dummy": baseline_pipeline,
    "HPO LogisticRegression": best_logreg,
    "HPO LinearSVC": best_svm
}

# Generating predictions
all_predictions = {name: model.predict(X_test) for name, model in all_model_pipelines.items()}

# Defining emotion labels
emotion_labels = sorted(df['emotion'].unique())

# PER CLASS CONFUSION MATRICES
def plot_confusion_per_class(model_name):
    """
    Plots a confusion matrix for a given model.
    Rows = true labels
    Columns = predicted labels
    """
    y_pred = all_predictions[model_name]
    cm = confusion_matrix(y_test, y_pred, labels=emotion_labels)

    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=emotion_labels, yticklabels=emotion_labels)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"{model_name} — Confusion Matrix")
    plt.show()

# Plotting confusion matrices
for model_name in all_model_pipelines:
    plot_confusion_per_class(model_name)

#  MOST CONFUSED EMOTION PAIRS
def most_confused_pairs(model_name, top_n=10):
    """
    Identifies emotion pairs that are most frequently confused
    """
    y_pred = all_predictions[model_name]

    cm = confusion_matrix(y_test, y_pred, labels=emotion_labels)
    # Removing correct predictions
    cm_off_diag = cm.copy()
    np.fill_diagonal(cm_off_diag, 0)

    pairs = []
    for i, true_em in enumerate(emotion_labels):
        for j, pred_em in enumerate(emotion_labels):
            if i != j and cm_off_diag[i,j] > 0:
                pairs.append(((true_em, pred_em), cm_off_diag[i,j]))

    # Sortting by frequency of confusion
    pairs = sorted(pairs, key=lambda x: x[1], reverse=True)

    print(f"\n MOST CONFUSED EMOTION PAIRS — {model_name} ")
    for (true_em, pred_em), count in pairs[:top_n]:
        print(f"True='{true_em}' → Predicted='{pred_em}' : {count}")

# Displaying confused pairs for each model
for model_name in all_model_pipelines:
    most_confused_pairs(model_name)

#  PER-CLASS METRICS

def per_class_metrics(model_name):
    """
    Computes precision, recall, F1-score, and support
    for each emotion class.
    """
    y_pred = all_predictions[model_name]

    p, r, f, s = precision_recall_fscore_support(y_test, y_pred, labels=emotion_labels, zero_division=0)
    df_metrics = pd.DataFrame({
        "emotion": emotion_labels,
        "precision": p,
        "recall": r,
        "f1": f,
        "support": s
    }).sort_values("f1")
    print(f"\n PER-CLASS METRICS — {model_name} ")
    display(df_metrics)
    return df_metrics

# Storing metrics for all models
all_per_class_metrics = {name: per_class_metrics(name) for name in all_model_pipelines}

#   ROW-LEVEL MISCLASSIFICATIONS

def build_error_table(model_name):
    """
    Builds a table of individual misclassified samples
    """
    y_pred = all_predictions[model_name]
    errors = []

    for i, (true_label, pred_label) in enumerate(zip(y_test, y_pred)):
        if true_label != pred_label:
            errors.append({
                "index": i,
                "text": X_test.iloc[i],
                "true": true_label,
                "pred": pred_label
            })

    df_errors = pd.DataFrame(errors).sort_values("index")
    print(f"\n ERROR TABLE — {model_name} ")
    print(f"Total misclassified samples: {len(df_errors)} / {len(y_test)}")
    display(df_errors.head(20))
    return df_errors

# Build error tables for all models
all_error_tables = {name: build_error_table(name) for name in all_model_pipelines}

#  INTERACTIVE ERROR INSPECTION

def inspect_errors(model_name, true_emotion=None, pred_emotion=None, n=5):
    """
    Allows manual inspection of specific error types
    """
    df_err = all_error_tables[model_name]
    if true_emotion:
        df_err = df_err[df_err["true"] == true_emotion]
    if pred_emotion:
        df_err = df_err[df_err["pred"] == pred_emotion]
    print(f"\nShowing top {n} errors for model='{model_name}' | true='{true_emotion}' | pred='{pred_emotion}'")
    display(df_err.head(n))
    return df_err.head(n)

#   SUMMARY
summary_df = pd.DataFrame([
    [name, f1_score(y_test, all_predictions[name], average="micro"),
     f1_score(y_test, all_predictions[name], average="macro")]
    for name in all_model_pipelines
], columns=["Model", "F1-Micro", "F1-Macro"])

print("\n FINAL PERFORMANCE SUMMARY ")
display(summary_df.sort_values("F1-Micro", ascending=False).reset_index(drop=True))

!pip install transformers datasets torch --quiet

# BERT TRANSFORMER MODEL

# BERT requires numeric labels so need to encode emotion strings as integers
from sklearn.preprocessing import LabelEncoder

print(" BERT TRANSFORMER MODEL ")

# Encoding
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_test_enc = label_encoder.transform(y_test)

num_labels = len(label_encoder.classes_) # Number of emotion classes

print("Emotion labels:", label_encoder.classes_)

from transformers import BertTokenizerFast, BertForSequenceClassification

model_name = "bert-base-uncased" # pre trained BERT model

# Tokenizer converts text into tokens
tokenizer = BertTokenizerFast.from_pretrained(model_name)

# Loading BERT with a classification head
bert_model = BertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)

# Tokenizing
def tokenize(texts):
    return tokenizer(
        texts.tolist(),
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

# Tokenize train and test data
X_train_tok = tokenize(X_train)
X_test_tok = tokenize(X_test)

import torch

# Convert encoded labels into PyTorch tensors
y_train_tensor = torch.tensor(y_train_enc)
y_test_tensor = torch.tensor(y_test_enc)

from torch.utils.data import TensorDataset, DataLoader

# Training dataset
train_dataset = TensorDataset(
    X_train_tok['input_ids'],
    X_train_tok['attention_mask'],
    y_train_tensor
)

# Test dataset
test_dataset = TensorDataset(
    X_test_tok['input_ids'],
    X_test_tok['attention_mask'],
    y_test_tensor
)
# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

from torch.optim import AdamW
from tqdm import tqdm

# Use GPU if not CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model.to(device)

optimizer = AdamW(bert_model.parameters(), lr=2e-5) # AdamW optimizer
epochs = 3 # Number of fine-tuning epochs

# Set model to training
bert_model.train()

for epoch in range(epochs):
    total_loss = 0

    # Progress bar
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")

    for batch in progress_bar:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()

        outputs = bert_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss # Compute loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())

    print(f"Epoch {epoch+1} average loss: {total_loss/len(train_loader):.4f}")

from sklearn.metrics import classification_report, f1_score, accuracy_score

bert_model.eval()

all_preds = []
all_true = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        outputs = bert_model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        logits = outputs.logits
        preds = torch.argmax(logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_true.extend(labels.cpu().numpy())

print("\nBERT RESULTS ")
print("Accuracy:", accuracy_score(all_true, all_preds))
print("Macro F1:", f1_score(all_true, all_preds, average="macro"))

print("\nClassification Report:")
print(classification_report(
    all_true,
    all_preds,
    target_names=label_encoder.classes_
))