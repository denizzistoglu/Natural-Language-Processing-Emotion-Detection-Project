# -*- coding: utf-8 -*-
"""GoEmotions_22soft1027.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FwYWpwifKwR4mawrna0qSlVS483jrQ6P

# **1. Demszky et al. (2020) - GoEmotions (Google Research)**
"""

#  Importing Core libraries

# Core numerical + data handling
import numpy as np
import pandas as pd

# Plotting / visualization
import matplotlib.pyplot as plt
import seaborn as sns

# File handling
import zipfile
import os

# Simple sentiment analysis
from textblob import TextBlob

# Text feature extraction
from sklearn.feature_extraction.text import CountVectorizer

# Utilities
from collections import Counter
import warnings
from itertools import combinations

warnings.filterwarnings('ignore')

# Displaying settings
pd.set_option('display.max_colwidth', 200)
sns.set(style='whitegrid')

# Uploading Files to colab
from google.colab import files
uploaded = files.upload()

# Unziping the Files
for filename in uploaded.keys():
    if filename.endswith(".zip"):
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall()

print(" Extracted files:", os.listdir())

# Loading and Combining CSV Files
df1 = pd.read_csv('goemotions_1.csv')
df2 = pd.read_csv('goemotions_2.csv')
df3 = pd.read_csv('goemotions_3.csv')

# Merging all splits into one dataframe
df = pd.concat([df1, df2, df3], ignore_index=True)
print(" combined shape:", df.shape)
df.head()

# Checking for missing values
print(df.isnull().sum())

# Dropping rows with missing text if any
df.dropna(subset=['text'], inplace=True)

# Basic cleaning
def preprocess(text_series):
    text_series = text_series.str.replace("(<br/>)", " ")
    text_series = text_series.str.replace('(<a).*(>).*(</a>)', ' ')
    text_series = text_series.str.replace('(&amp)', '&')
    text_series = text_series.str.replace('(&gt)', '>')
    text_series = text_series.str.replace('(&lt)', '<')
    text_series = text_series.str.replace('(\xa0)', ' ')
    return text_series
df['text'] = preprocess(df['text'])
print("cleaned text sample:")
df['text'].head()

# Sentiment Polarity and Text Stats

df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))
df['char_count'] = df['text'].apply(lambda x: len(str(x)))

df[['text', 'polarity', 'word_count', 'char_count']].head()

# All emotion label columns
emotion_cols = ['admiration', 'anger', 'annoyance', 'approval', 'caring', 'confusion',
                'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',
                'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',
                'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',
                'remorse', 'sadness', 'surprise', 'neutral']

#  Melting the dataframe for multi-label handling
#  Makes it easier to count and plot emotions
df_melted = df.melt(
    id_vars=[],                 # No other id columns needed for counting
    value_vars=emotion_cols,    # Columns to unpivot
    var_name='emotion',         # Name of the new column for emotion names
    value_name='value'          # Name of the new column for values
)

# Keep only the emotions present (value == 1)
df_melted = df_melted[df_melted['value'] == 1]

#  Ploting the distributionof how often each emotion appears
plt.figure(figsize=(10,6))
sns.countplot(
    y='emotion',
    data=df_melted,
    order=df_melted['emotion'].value_counts().index,
    palette='viridis'
)
plt.title('Emotion Label Distribution', fontsize=16)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Emotion', fontsize=12)
plt.tight_layout()
plt.show()

# Distribution of Polarity & Text Length
features = ['polarity', 'word_count', 'char_count']
titles = ['Polarity Distribution', 'Word Count Distribution', 'Character Count Distribution']
colors = ['#9966ff', '#3399ff', '#ff6600']

for feature, title, color in zip(features, titles, colors):
    sns.histplot(df[feature], bins=50, color=color, kde=True)
    plt.title(title, size=15)
    plt.xlabel(feature)
    plt.show()

# Example Texts by Polarity

print(" Positive samples:")
for text in df.loc[df.polarity > 0.5, 'text'].sample(5).values:
    print('-', text)

print("\n Neutral samples:")
for text in df.loc[(df.polarity > -0.1) & (df.polarity < 0.1), 'text'].sample(5).values:
    print('-', text)

print("\n Negative samples:")
for text in df.loc[df.polarity < -0.5, 'text'].sample(5).values:
    print('-', text)

# N-gram Analysis

def get_top_ngrams(corpus, ngram_range, stop_words=None, n=None):
    vec = CountVectorizer(stop_words=stop_words, ngram_range=ngram_range).fit(corpus)
    bag = vec.transform(corpus)
    sum_words = bag.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    common = words_freq[:n]
    return pd.DataFrame(common, columns=['word', 'freq'])

# Extract top unigrams, bigrams, trigrams
n = 20
unigrams = get_top_ngrams(df['text'], (1,1), stop_words='english', n=n)
bigrams = get_top_ngrams(df['text'], (2,2), stop_words='english', n=n)
trigrams = get_top_ngrams(df['text'], (3,3), stop_words='english', n=n)

plt.figure(figsize=(8,10))
sns.barplot(x='freq', y='word', data=unigrams, color='#9966ff')
plt.title('Top 20 Unigrams (Stopwords Removed)', size=15)
plt.show()

plt.figure(figsize=(8,10))
sns.barplot(x='freq', y='word', data=bigrams, color='#3399ff')
plt.title('Top 20 Bigrams (Stopwords Removed)', size=15)
plt.show()

plt.figure(figsize=(8,10))
sns.barplot(x='freq', y='word', data=trigrams, color='#ff6600')
plt.title('Top 20 Trigrams (Stopwords Removed)', size=15)
plt.show()

# Polarity vs Label

# Melt emotion columns
df_melted = df.melt(
    id_vars=['polarity'],
    value_vars=emotion_cols,
    var_name='emotion',
    value_name='value'
)

# Keep only rows with value == 1
df_melted = df_melted[df_melted['value'] == 1]

# Polarity vs Emotion
plt.figure(figsize=(12,6))
sns.boxplot(x='emotion', y='polarity', data=df_melted, palette='viridis')
plt.title('Sentiment Polarity vs Emotion', size=15)
plt.xticks(rotation=45)
plt.show()

# Summary
df_melted = df.melt(
    id_vars=[],
    value_vars=emotion_cols,
    var_name='emotion',
    value_name='value'
)

# Keep only rows with value == 1
df_melted = df_melted[df_melted['value'] == 1]

print("Total emotion entries:", len(df_melted))
print("Number of unique emotions:", df_melted['emotion'].nunique())
print("\nSample rows:")
df_melted.sample(27)

# Requirements
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_recall_fscore_support
from scipy.stats import uniform, loguniform
import numpy as np
import pandas as pd
import time

# Preparing the data
def row_to_labels(row):
    return [emotion for emotion in emotion_cols if row[emotion] == 1]

if 'targets' not in df.columns:
    df['targets'] = df.apply(row_to_labels, axis=1)

from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer(classes=emotion_cols)
y_all = mlb.fit_transform(df['targets'])
X_all = df['text']

# reset index
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X_all, y_all, test_size=0.20, random_state=42, stratify=None
)
X_train_raw = X_train_raw.reset_index(drop=True)
X_test_raw = X_test_raw.reset_index(drop=True)

print("Train size:", len(X_train_raw), "Test size:", len(X_test_raw))


tfidf_params = dict(max_features=30000, ngram_range=(1,2), min_df=2)


# Baseline: TF-IDF + OneVsRest

baseline_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(**tfidf_params)),
    ('clf', OneVsRestClassifier(LogisticRegression(max_iter=2000, C=1.0, solver='liblinear', random_state=42)))
])

t0 = time.time()
baseline_pipeline.fit(X_train_raw, y_train)
t1 = time.time()
print(f"\nBaseline trained in {t1-t0:.1f}s")

y_pred_base = baseline_pipeline.predict(X_test_raw)
print("\n BASELINE LOGISTIC REGRESSION ")
print("F1-Micro:", f1_score(y_test, y_pred_base, average="micro"))
print("F1-Macro:", f1_score(y_test, y_pred_base, average="macro"))

# Requirements
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_recall_fscore_support
from scipy.stats import uniform, loguniform
import numpy as np
import pandas as pd
import time

# Preparing multi label targets
def row_to_labels(row):
    return [emotion for emotion in emotion_cols if row[emotion] == 1] # collect all emotions marked as 1 for this row

# create target column if it doesn't already exist
if 'targets' not in df.columns:
    df['targets'] = df.apply(row_to_labels, axis=1)

# label lists into multi-hot vectors
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer(classes=emotion_cols)

y_all = mlb.fit_transform(df['targets'])
X_all = df['text']

# train / test split
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X_all, y_all, test_size=0.20, random_state=42, stratify=None # multi-label so no stratification
)
# reset index
X_train_raw = X_train_raw.reset_index(drop=True)
X_test_raw = X_test_raw.reset_index(drop=True)

print("Train size:", len(X_train_raw), "Test size:", len(X_test_raw))

# shared TF-IDF settings
tfidf_params = dict(max_features=30000, ngram_range=(1,2), min_df=2)


# Baseline: TF-IDF + OneVsRest
baseline_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(**tfidf_params)), # convert text to TF-IDF features
    ('clf', OneVsRestClassifier(LogisticRegression(max_iter=2000, C=1.0, solver='liblinear', random_state=42))) # one-vs-rest for multi-label classification
])

# training baseline model
t0 = time.time()
baseline_pipeline.fit(X_train_raw, y_train)
t1 = time.time()

print(f"\nBaseline trained in {t1-t0:.1f}s")

# evaluating the baseline
y_pred_base = baseline_pipeline.predict(X_test_raw)
print("\n BASELINE LOGISTIC REGRESSION ")
print("F1-Micro:", f1_score(y_test, y_pred_base, average="micro"))
print("F1-Macro:", f1_score(y_test, y_pred_base, average="macro"))

# Classifier comparison

print(" CLASSIFIER COMPARISON ")
classifiers = {
    "Logistic Regression": OneVsRestClassifier(
        LogisticRegression(
            max_iter=2000,
            solver='liblinear',
            random_state=42,
            class_weight='balanced'
        )
    ),

    "Linear SVM": OneVsRestClassifier(
        LinearSVC(
            random_state=42,
            class_weight='balanced'
        )
    ),

    "Multinomial NB": OneVsRestClassifier(
        MultinomialNB()
    )
}

comparison_results = []
for name, clf in classifiers.items():
    # build pipeline per model
    pipe = Pipeline([('tfidf', TfidfVectorizer(**tfidf_params)), ('clf', clf)])

    t0 = time.time()
    pipe.fit(X_train_raw, y_train)
    t1 = time.time()

    y_pred = pipe.predict(X_test_raw)

    f1_micro = f1_score(y_test, y_pred, average='micro')
    f1_macro = f1_score(y_test, y_pred, average='macro')
    elapsed = t1 - t0

    print(f"{name}: F1-micro={f1_micro:.4f}, F1-macro={f1_macro:.4f}, time={elapsed:.1f}s")

    comparison_results.append([name, f1_micro, f1_macro, elapsed])

comp_df = pd.DataFrame(comparison_results, columns=['Model', 'F1-Micro', 'F1-Macro', 'Time_s'])

print("\nClassifier comparison summary:")
print(comp_df.sort_values('F1-Micro', ascending=False).reset_index(drop=True))

# hyperparameter optimization imports
from scipy.stats import loguniform
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
import time

# hyperparameter optimization
print("\n HYPERPARAMETER OPTIMIZATION (HPO) ")

# LOGISTIC REGRESSION HPO

logreg_pipeline = Pipeline([
    # converts text into TF-IDF features
    ('tfidf', TfidfVectorizer(
        stop_words='english'
    )),
    # one-vs-rest logistic regression for multi-label setup
    ('clf', OneVsRestClassifier(
        LogisticRegression(max_iter=2000, random_state=42)
    ))
])
# parameter search space
logreg_param_dist = {
    'clf__estimator__C': loguniform(1e-3, 10), # regularization strength
    'tfidf__max_features': [10000, 20000, 30000], # TF-IDF feature size
    'tfidf__ngram_range': [(1,1),(1,2)] # unigram vs unigram & bigram
}

# randomized search for faster exploration
logreg_search = RandomizedSearchCV(
    logreg_pipeline,
    param_distributions=logreg_param_dist,
    n_iter=10,
    scoring='f1_micro',
    cv=3,
    random_state=42,
    verbose=1,
    n_jobs=-1
)

# running HPO
t0 = time.time()
logreg_search.fit(X_train_raw, y_train)
t1 = time.time()

print(f"\nLogReg HPO took {t1-t0:.1f}s")
print("Best params (Logistic Regression):", logreg_search.best_params_)

# best tuned logistic regression model
best_logreg = logreg_search.best_estimator_

# evaluate tuned logistic regression
y_pred_logreg = best_logreg.predict(X_test_raw)

print("\n HPO Logistic Regression results ")
print("F1-Micro:", f1_score(y_test, y_pred_logreg, average='micro'))
print("F1-Macro:", f1_score(y_test, y_pred_logreg, average='macro'))

# LINEARSVC HPO
print("Running HPO for LinearSVC")

svm_pipeline = Pipeline([
    # TF-IDF text vectorization
    ('tfidf', TfidfVectorizer(
        stop_words='english'
    )),
    # linear SVM wrapped in one-vs-rest
    ('clf', OneVsRestClassifier(
        LinearSVC(random_state=42)
    ))
])

# parameter search space for SVM
svm_param_dist = {
    'clf__estimator__C': loguniform(1e-3, 10), # margin regularization

    # Full TF-IDF search
    'tfidf__max_features': [15000, 30000],
    'tfidf__ngram_range': [(1,1), (1,2)]
}

svm_search = RandomizedSearchCV(
    svm_pipeline,
    param_distributions=svm_param_dist,
    n_iter=8,
    scoring='f1_micro',
    cv=2,
    random_state=42,
    verbose=1,
    n_jobs=-1
)
# running HPO
t0 = time.time()
svm_search.fit(X_train_raw, y_train)
t1 = time.time()

print(f"\nLinearSVC HPO took {t1-t0:.1f}s")
print("Best params (LinearSVC):", svm_search.best_params_)

# best tuned SVM model
best_svm = svm_search.best_estimator_

# evaluate tuned SVM
y_pred_svm = best_svm.predict(X_test_raw)

print("\nHPO LinearSVC results ")
print("F1-Micro:", f1_score(y_test, y_pred_svm, average='micro'))
print("F1-Macro:", f1_score(y_test, y_pred_svm, average='macro'))

# Error analysis

def run_error_analysis(model, model_name, X_test_raw, y_test):
    print(f"\n ERROR ANALYSIS: {model_name} ")
    # ensure y_test is numpy array
    y_test_arr = y_test if isinstance(y_test, np.ndarray) else y_test.values
    # Predict
    y_pred = model.predict(X_test_raw)
    # Metrics
    metrics = {
        'accuracy': accuracy_score(y_test_arr, y_pred),
        'f1_micro': f1_score(y_test_arr, y_pred, average='micro', zero_division=0),
        'f1_macro': f1_score(y_test_arr, y_pred, average='macro', zero_division=0)
    }

    print(pd.DataFrame([metrics]))

    # Per class precision/recall/f1
    prf = precision_recall_fscore_support(y_test_arr, y_pred, average=None, zero_division=0)
    per_class_df = pd.DataFrame({
        'class': mlb.classes_,
        'precision': prf[0],
        'recall': prf[1],
        'f1': prf[2],
        'support': prf[3]
    }).sort_values('f1')
    print("\nWorst 10 classes by F1:\n", per_class_df.head(10))

    # find misclassified examples
    errors = []
    for i in range(len(y_test_arr)):
        true_i = y_test_arr[i]
        pred_i = y_pred[i]
        num_mistakes = int(np.sum(true_i != pred_i))
        if num_mistakes > 0:
            errors.append({
                'index': i,
                'text': X_test_raw.iat[i],
                'true_label_indices': np.where(true_i==1)[0].tolist(),
                'pred_label_indices': np.where(pred_i==1)[0].tolist(),
                'num_mistakes': num_mistakes
            })

    errors_df = pd.DataFrame(errors).sort_values('num_mistakes', ascending=False).reset_index(drop=True)
    print(f"Total misclassified test rows: {len(errors_df)} / {len(y_test_arr)}")
    return errors_df, per_class_df, metrics

# Run error analysis for each model
baseline_errors, baseline_perclass, baseline_metrics = run_error_analysis(
    baseline_pipeline, "Baseline Logistic Regression", X_test_raw, y_test
)

hpo_logreg_errors, hpo_logreg_perclass, hpo_logreg_metrics = run_error_analysis(
    best_logreg, "HPO Logistic Regression", X_test_raw, y_test
)

hpo_svm_errors, hpo_svm_perclass, hpo_svm_metrics = run_error_analysis(
    best_svm, "HPO Linear SVM", X_test_raw, y_test
)

# Save top misclassifications for inspection
baseline_errors.head(20).to_csv('baseline_misclassified_samples.csv', index=False)
hpo_logreg_errors.head(20).to_csv('hpo_logreg_misclassified_samples.csv', index=False)
hpo_svm_errors.head(20).to_csv('hpo_svm_misclassified_samples.csv', index=False)

print("\nSaved CSVs: baseline_misclassified_samples.csv, hpo_logreg_misclassified_samples.csv, hpo_svm_misclassified_samples.csv")

# summary
summary = pd.DataFrame([
    ['Baseline LogisticRegression', baseline_metrics['f1_micro'], baseline_metrics['f1_macro']],
    ['HPO LogisticRegression', hpo_logreg_metrics['f1_micro'], hpo_logreg_metrics['f1_macro']],
    ['HPO LinearSVC', hpo_svm_metrics['f1_micro'], hpo_svm_metrics['f1_macro']]
], columns=['Model','F1-Micro','F1-Macro'])

print("\n FINAL SUMMARY ")
print(summary.sort_values('F1-Micro', ascending=False).reset_index(drop=True))

# ADVANCED MULTI-LABEL ERROR ANALYSIS

# extra metrics
from sklearn.metrics import multilabel_confusion_matrix, precision_recall_fscore_support
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# compute predictions for all comparison models
all_model_pipelines = {
    "Baseline LogReg": baseline_pipeline,
    "HPO LogisticRegression": best_logreg,
    "HPO LinearSVC": best_svm
}

all_predictions = {name: model.predict(X_test_raw) for name, model in all_model_pipelines.items()}

# ONE-VS-REST CONFUSION MATRICES (PER EMOTION)
# For multi-label classification, sklearn gives (TP, FP, FN, TN) for each class separately.

# plots a 2x2 confusion matrix for a single emotion
def plot_confusion_for_class(model_name, class_index):
    y_pred = all_predictions[model_name]
    # sklearn returns one confusion matrix per label
    mcm = multilabel_confusion_matrix(y_test, y_pred)
    cm = mcm[class_index]  # 2x2 matrix: [[TN FP], [FN TP]]

    plt.figure(figsize=(4,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{model_name} — Confusion Matrix for '{emotion_cols[class_index]}'")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

# visualize confusion for first 5 emotions
for model_name in all_model_pipelines:
    print(f"\nShowing confusion matrices for: {model_name}")
    for idx in range(5):
        plot_confusion_for_class(model_name, idx)

# MOST CONFUSED EMOTION PAIRS

# finds which emotions are commonly missed
def compute_label_pair_confusions(y_true, y_pred):
    fn_pairs = Counter()
    fp_pairs = Counter()

    for t, p in zip(y_true, y_pred):
        t_idx = np.where(t == 1)[0]
        p_idx = np.where(p == 1)[0]
        # false negatives and false positives
        fn = set(t_idx) - set(p_idx)
        fp = set(p_idx) - set(t_idx)

        # count confusion pairs
        for a in fn:
            for b in fp:
                fn_pairs[(emotion_cols[a], emotion_cols[b])] += 1

    return fn_pairs, fp_pairs

# display most common confusion patterns
for model_name in all_predictions:
    print(f"\n MOST CONFUSED LABEL PAIRS — {model_name} ")
    y_pred = all_predictions[model_name]

    fn_pairs, fp_pairs = compute_label_pair_confusions(y_test, y_pred)

    top_fn = fn_pairs.most_common(10)
    if top_fn:
        print("\nCommon Missing vs Incorrectly Added (FN → FP):")
        for (missing, added), count in top_fn:
            print(f"  Missing '{missing}' → Added '{added}' : {count}")
    else:
        print("No label pair errors found")


# PER-LABEL PRECISION, RECALL, F1 TABLE

# builds a per-emotion metrics table
def per_label_metrics(model_name):
    y_pred = all_predictions[model_name]
    p, r, f, s = precision_recall_fscore_support(
        y_test, y_pred, average=None, zero_division=0
    )
    df_metrics = pd.DataFrame({
        "emotion": emotion_cols,
        "precision": p,
        "recall": r,
        "f1": f,
        "support": s
    }).sort_values("f1")

    print(f"\nPER-LABEL METRICS — {model_name} ")
    display(df_metrics.head(10))  # Worst classes
    return df_metrics

# compute per-label metrics for all models
all_per_label_metrics = {
    name: per_label_metrics(name) for name in all_predictions
}

# ROW-LEVEL MISCLASSIFICATION TABLE

# builds a table of individual prediction errors
def build_error_table(model_name):
    y_pred = all_predictions[model_name]
    errors = []

    for i in range(len(y_test)):
        t = y_test[i]
        p = y_pred[i]

        t_idx = np.where(t == 1)[0]
        p_idx = np.where(p == 1)[0]

        missing = sorted(list(set(t_idx) - set(p_idx)))
        extra = sorted(list(set(p_idx) - set(t_idx)))

        if missing or extra:
            errors.append({
                "index": i,
                "text": X_test_raw.iloc[i],
                "true": [emotion_cols[k] for k in t_idx],
                "pred": [emotion_cols[k] for k in p_idx],
                "missing_labels": [emotion_cols[k] for k in missing],
                "extra_labels": [emotion_cols[k] for k in extra],
                "num_errors": len(missing) + len(extra)
            })

    df_errors = pd.DataFrame(errors).sort_values("num_errors", ascending=False)
    print(f"\n ERROR TABLE — {model_name} ")
    print(f"Total incorrect predictions: {len(df_errors)} / {len(y_test)}")
    display(df_errors.head(20))  # preview

    return df_errors

# generate error tables for all models
all_error_tables = {
    name: build_error_table(name) for name in all_predictions
}

# INTERACTIVE ERROR INSPECTION TOOL

# quick utility to inspect specific error types
def inspect_examples(model_name, emotion=None, error_type="missing", n=5):
    """
    model_name: name of model (string)
    emotion: specific emotion label to filter by; None = show all
    error_type: "missing" or "extra"
    n: number of rows to show
    """
    df = all_error_tables[model_name]

    if emotion:
        if error_type == "missing":
            df = df[df["missing_labels"].apply(lambda lst: emotion in lst)]
        else:
            df = df[df["extra_labels"].apply(lambda lst: emotion in lst)]

    print(f"\nExamples for model={model_name}, emotion={emotion}, error_type={error_type}")
    return df.head(n)


# SUMMARY OF MODEL PERFORMANCE

summary_df = pd.DataFrame([
    ["Baseline LogReg",
     f1_score(y_test, all_predictions["Baseline LogReg"], average="micro"),
     f1_score(y_test, all_predictions["Baseline LogReg"], average="macro")],
    ["HPO LogisticRegression",
     f1_score(y_test, all_predictions["HPO LogisticRegression"], average="micro"),
     f1_score(y_test, all_predictions["HPO LogisticRegression"], average="macro")],
    ["HPO LinearSVC",
     f1_score(y_test, all_predictions["HPO LinearSVC"], average="micro"),
     f1_score(y_test, all_predictions["HPO LinearSVC"], average="macro")]
], columns=["Model", "F1-Micro", "F1-Macro"])

print("\n===== FINAL PERFORMANCE SUMMARY =====")
display(summary_df.sort_values("F1-Micro", ascending=False).reset_index(drop=True))

!pip install transformers datasets torch accelerate -q
!pip install -U transformers

# BERT / TRANSFORMER BASED MULTI LABEL CLASSIFICATION

# installing required libraries
!pip install transformers accelerate -q

# imports
import torch # core deep learning library
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments # HuggingFace transformer utilities
from torch.utils.data import  Dataset  # PyTorch dataset base class

# MODEL CONFIGURATION

# number of emotion labels
num_labels = y_train.shape[1]

# loads pretrained tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# custom dataset for multi label text classification
class GoEmotionsDataset(Dataset):

    def __init__(self, texts, labels):
        self.texts = list(texts) # store raw text
        self.labels = torch.tensor(labels, dtype=torch.float32) # converts labels to float tensor

    def __len__(self):
        return len(self.texts) # total number of samples

    def __getitem__(self, idx):
        # tokenizing a single text sample
        enc = tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=128,
            return_tensors="pt"
        )
        # removing batch dimension added by tokenizer
        item = {k: v.squeeze(0) for k, v in enc.items()}
        # attach label vector
        item["labels"] = self.labels[idx]

        return item

# building train and test datasets
train_ds = GoEmotionsDataset(X_train_raw, y_train)
test_ds = GoEmotionsDataset(X_test_raw, y_test)

# load pretrained BERT model
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels,
    problem_type="multi_label_classification"
)

# training configuration
training_args = TrainingArguments(
    output_dir="./bert-goemotions",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    num_train_epochs=2,
    weight_decay=0.01,
    report_to="none", # disables wandb
    logging_steps=200
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds
)

trainer.train() # train the model

# generate predictions on test set
preds = trainer.predict(test_ds).predictions # raw logits from the model
pred_probs = torch.sigmoid(torch.tensor(preds)) # converts logits to probabilities
y_pred_bert = (pred_probs > 0.5).int().numpy() # applies threshold to get binary predictions

# EVALUATION
from sklearn.metrics import f1_score

print("\nBERT RESULTS")
print("F1-micro:", f1_score(y_test, y_pred_bert, average="micro"))
print("F1-macro:", f1_score(y_test, y_pred_bert, average="macro"))