# -*- coding: utf-8 -*-
"""Lexicon_22soft1027.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14khpUEVVXO2Nkh0l4rlI2giiNbPsB_AK
"""

# Imports
import io
import os
import time
import warnings
from itertools import combinations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from textblob import TextBlob

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import (
    classification_report,
    f1_score,
    accuracy_score,
    precision_recall_fscore_support
)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform

from google.colab import files

warnings.filterwarnings("ignore") # Suppressing warnings
sns.set(style="whitegrid")
RANDOM_STATE = 42

# LOADING THE DATASET

print("Upload the 10 NRC emotion files")
uploaded = files.upload()

# Lexicon emotion categories
emotions = [
    'anger', 'anticipation', 'disgust', 'fear', 'joy',
    'negative', 'positive', 'sadness', 'surprise', 'trust'
]

dfs = []

# extracts words with label = 1 for each emotion
for filename in uploaded:
    for emotion in emotions:
        if emotion in filename.lower():
            lines = io.BytesIO(uploaded[filename]).read().decode("utf-8").splitlines()
            words = [l.split('\t')[0].lower().strip() for l in lines if l.endswith('\t1')]
            dfs.append(pd.DataFrame({"word": words, "emotion": emotion}))
            print(f"Loaded {emotion}: {len(words)} words")
            break

# Combines all emotion files into a single dataframe
df = pd.concat(dfs, ignore_index=True).drop_duplicates()
print("\nTotal (word, emotion) pairs:", len(df))

# EXPLORATORY DATA ANALYSIS (EDA)

# Distribution of words per emotion
plt.figure(figsize=(10,5))
sns.countplot(data=df, x="emotion", order=df["emotion"].value_counts().index)
plt.title("Word Count per Emotion")
plt.xticks(rotation=45)
plt.show()

# Number of emotions assigned to each word
word_emotion_counts = df.groupby("word")["emotion"].nunique()

plt.figure(figsize=(8,5))
sns.histplot(word_emotion_counts, bins=20)
plt.title("Number of Emotions per Word")
plt.xlabel("Emotion Count")
plt.show()

print("Average emotions per word:", word_emotion_counts.mean())

# Compute overlap between emotion vocabularies
emotion_sets = {
    e: set(df[df.emotion == e].word) for e in emotions
}

overlap_matrix = pd.DataFrame(index=emotions, columns=emotions)

for e1 in emotions:
    for e2 in emotions:
        overlap_matrix.loc[e1, e2] = len(emotion_sets[e1] & emotion_sets[e2])

plt.figure(figsize=(10,8))
sns.heatmap(overlap_matrix.astype(int), cmap="Blues", annot=False)
plt.title("Emotion Word Overlap Heatmap")
plt.show()

# Emotion Pairwise Overlap Analysis

from itertools import combinations

# Build sets of words per emotion
emotion_sets = {
    emotion: set(df[df["emotion"] == emotion]["word"])
    for emotion in df["emotion"].unique()
}

# Compute pairwise overlaps
overlap_records = []

for e1, e2 in combinations(emotion_sets.keys(), 2):
    shared_words = emotion_sets[e1] & emotion_sets[e2]
    overlap_records.append({
        "Emotion Pair": f"{e1} & {e2}",
        "Shared Words": len(shared_words)
    })

overlap_df = (
    pd.DataFrame(overlap_records)
    .sort_values("Shared Words", ascending=False)
)

# Display top overlaps
display(overlap_df.head(10))

# Plot top N emotion overlaps
TOP_N = 10

plt.figure(figsize=(10, 5))
sns.barplot(
    data=overlap_df.head(TOP_N),
    x="Shared Words",
    y="Emotion Pair",
    orient="h"
)
plt.title("Top Emotion Word Overlaps")
plt.xlabel("Number of Shared Words")
plt.ylabel("Emotion Pairs")
plt.show()

# Polarity Analysis
# Attach polarity to word-emotion pairs
df_pol = df.merge(
    lex_df[["word", "polarity", "subjectivity"]],
    on="word",
    how="left"
)
# Polarity distribution per emotion
plt.figure(figsize=(10,5))
sns.boxplot(
    data=df_pol,
    x="emotion",
    y="polarity",
    order=df_pol["emotion"].value_counts().index
)
plt.xticks(rotation=45)
plt.title("Polarity Distribution by Emotion")
plt.show()

# DATA PREPARATION

X = lex_df["word"]

# Multi-label targets are emotion sets per word
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(lex_df["emotion"])

# Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

print("Classes:", mlb.classes_)

def deep_model_analysis(
    model_name,
    y_true,
    y_pred,
    classes,
    X_test=None,
    top_k_errors=10
):
    """
    Performs error analysis for a multi-label classifier
    """
    print(f"\n{model_name}")

    # Overall Metrics
    print("\nOverall Performance:")
    print("F1 Micro:", f1_score(y_true, y_pred, average="micro"))
    print("F1 Macro:", f1_score(y_true, y_pred, average="macro"))
    print("Subset Accuracy:", accuracy_score(y_true, y_pred))

    #  Per-Label Classification Report
    print("\nClassification Report (Per Emotion):")
    print(classification_report(
        y_true,
        y_pred,
        target_names=classes,
        zero_division=0
    ))

    # Prediction Bias Analysis
    label_support = y_true.sum(axis=0)
    label_predictions = y_pred.sum(axis=0)

    bias_df = pd.DataFrame({
        "Emotion": classes,
        "True Support": label_support,
        "Predicted Count": label_predictions,
        "Prediction / True Ratio": np.round(
            label_predictions / np.maximum(label_support, 1), 2
        )
    }).sort_values("Prediction / True Ratio", ascending=False)

    print("\nPrediction Bias (Over/Under Prediction):")
    display(bias_df)

    # Error Density
    errors_per_sample = np.sum(y_true != y_pred, axis=1)
    print("\nError Density:")
    print(pd.Series(errors_per_sample).value_counts().sort_index())

# BASELINE MODELS

# Baseline 1: Majority Label Baseline
# Predicts the most frequent emotions for every instance
label_freq = y_train.mean(axis=0)
majority_threshold = 0.5 * label_freq.max()

majority_pred = (np.tile(label_freq, (len(y_test), 1)) >= majority_threshold).astype(int)

deep_model_analysis(
    model_name="Majority Baseline",
    y_true=y_test,
    y_pred=majority_pred,
    classes=mlb.classes_,
    X_test=X_test
)

# Baseline 2: CountVectorizer + Logistic Regression
# Tests if TF-IDF actually helps more than raw counts
from sklearn.feature_extraction.text import CountVectorizer

count_baseline = Pipeline([
    ("vec", CountVectorizer(analyzer="char_wb", ngram_range=(2,5))),
    ("clf", OneVsRestClassifier(LogisticRegression(max_iter=2000)))
])

count_baseline.fit(X_train, y_train)
count_preds = count_baseline.predict(X_test)

deep_model_analysis(
    model_name="CountVectorizer + Logistic Regression",
    y_true=y_test,
    y_pred=count_preds,
    classes=mlb.classes_,
    X_test=X_test
)

# CLASSIFIER COMPARISON

# Shared TF-IDF configuration
tfidf = TfidfVectorizer(
    analyzer="char_wb",
    ngram_range=(2,5),
    min_df=2
)
models = {
    "Logistic Regression": LogisticRegression(max_iter=2000, class_weight="balanced"),
    "Linear SVM": LinearSVC(class_weight="balanced"),
    "Multinomial NB": MultinomialNB()
}
results = []

# Train and evaluate for each classifier
for name, clf in models.items():
    pipe = Pipeline([
        ("tfidf", tfidf),
        ("clf", OneVsRestClassifier(clf))
    ])

    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)

    # Store summary metrics
    f1_micro = f1_score(y_test, preds, average="micro")
    f1_macro = f1_score(y_test, preds, average="macro")
    results.append([name, f1_micro, f1_macro])

    # error analysis
    deep_model_analysis(
        model_name=name,
        y_true=y_test,
        y_pred=preds,
        classes=mlb.classes_,
        X_test=X_test
    )


results_df = pd.DataFrame(
    results, columns=["Model", "F1-Micro", "F1-Macro"]
).sort_values("F1-Micro", ascending=False)

results_df

# HYPERPARAMETER OPTIMIZATION (HPO)

# Logistic Regression HPO
logreg_hpo = Pipeline([
    ("tfidf", TfidfVectorizer()),
    ("clf", OneVsRestClassifier(
        LogisticRegression(max_iter=2000, class_weight="balanced")
    ))
])

logreg_param_dist = {
    "tfidf__analyzer": ["char", "char_wb"],
    "tfidf__ngram_range": [(2,4), (2,5), (3,6)],
    "tfidf__max_features": [5000, 10000, 20000],
    "clf__estimator__C": loguniform(1e-3, 10)
}

logreg_search = RandomizedSearchCV(
    logreg_hpo,
    logreg_param_dist,
    n_iter=15,
    scoring="f1_micro",
    cv=3,
    n_jobs=-1,
    verbose=1,
    random_state=RANDOM_STATE
)

logreg_search.fit(X_train, y_train)
best_logreg = logreg_search.best_estimator_

# Linear SVM HPO
svm_hpo = Pipeline([
    ("tfidf", TfidfVectorizer()),
    ("clf", OneVsRestClassifier(
        LinearSVC(class_weight="balanced")
    ))
])

svm_param_dist = {
    "tfidf__analyzer": ["char", "char_wb"],
    "tfidf__ngram_range": [(2,4), (2,5)],
    "tfidf__max_features": [5000, 15000],
    "clf__estimator__C": loguniform(1e-3, 10)
}

svm_search = RandomizedSearchCV(
    svm_hpo,
    svm_param_dist,
    n_iter=10,
    scoring="f1_micro",
    cv=3,
    n_jobs=-1,
    verbose=1,
    random_state=RANDOM_STATE
)

svm_search.fit(X_train, y_train)
best_svm = svm_search.best_estimator_

deep_model_analysis(
    "HPO Logistic Regression",
    y_test,
    best_logreg.predict(X_test),
    mlb.classes_,
    X_test
)

deep_model_analysis(
    "HPO Linear SVM",
    y_test,
    best_svm.predict(X_test),
    mlb.classes_,
    X_test
)

# ERROR ANALYSIS

all_models = { # Collecting predictions for every model
    "Majority Baseline": majority_pred,
    "CountVectorizer Baseline": count_preds,
}

# Default models
for name, clf in models.items():
    pipe = Pipeline([
        ("tfidf", tfidf),
        ("clf", OneVsRestClassifier(clf))
    ])
    pipe.fit(X_train, y_train)
    all_models[name] = pipe.predict(X_test)

# HPO models
all_models["HPO Logistic Regression"] = best_logreg.predict(X_test)
all_models["HPO Linear SVM"] = best_svm.predict(X_test)

# Per-emotion metrics
def per_emotion_metrics(y_true, y_pred, classes):
    """Compute precision, recall, and F1 for each emotion."""
    pr, rc, f1, sup = precision_recall_fscore_support(
        y_true, y_pred, average=None, zero_division=0
    )
    return pd.DataFrame({
        "Emotion": classes,
        "Precision": pr,
        "Recall": rc,
        "F1": f1,
        "Support": sup
    }).sort_values("F1")
per_emotion_reports = {}

for name, preds in all_models.items():
    report = per_emotion_metrics(y_test, preds, mlb.classes_)
    per_emotion_reports[name] = report

    print(f"\n Worst emotions — {name}")
    display(report.head(5))

# False Positive vs False Negative analysis
def fp_fn_counts(y_true, y_pred, classes):
    """Count false positives and false negatives per emotion."""
    rows = []
    for i, emotion in enumerate(classes):
        fp = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 1))
        fn = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 0))
        rows.append([emotion, fp, fn])
    return pd.DataFrame(rows, columns=["Emotion", "False Positives", "False Negatives"])

for name, preds in all_models.items():
    print(f"\n{name} — FP vs FN")
    display(fp_fn_counts(y_test, preds, mlb.classes_)
            .sort_values("False Negatives", ascending=False))

# Word-level error inspection
def collect_errors(X_test, y_true, y_pred, classes):
    """Collect misclassified words with true vs predicted emotions."""
    errors = []
    for i in range(len(X_test)):
        if not np.array_equal(y_true[i], y_pred[i]):
            errors.append({
                "word": X_test.iloc[i],
                "true": [classes[j] for j in np.where(y_true[i] == 1)[0]],
                "pred": [classes[j] for j in np.where(y_pred[i] == 1)[0]],
                "num_errors": np.sum(y_true[i] != y_pred[i])
            })
    return pd.DataFrame(errors)

error_tables = {}

for name, preds in all_models.items():
    error_tables[name] = collect_errors(X_test, y_test, preds, mlb.classes_)

    print(f"\n{name} — sample errors")
    display(error_tables[name].head(10))

# Cross model error words
error_word_sets = [
    set(df["word"]) for df in error_tables.values()
]

common_error_words = set.intersection(*error_word_sets)

print("Words misclassified by ALL models:")
list(common_error_words)[:30]

# Emotion confusion patterns
from collections import Counter

def emotion_confusions(y_true, y_pred, classes):
    """Count how often one emotion is confused with another."""
    confusions = Counter()
    for i in range(len(y_true)):
        true_set = set(np.where(y_true[i] == 1)[0])
        pred_set = set(np.where(y_pred[i] == 1)[0])
        for t in true_set:
            for p in pred_set:
                if t != p:
                    confusions[(classes[t], classes[p])] += 1
    return confusions
conf = emotion_confusions(
    y_test,
    all_models["HPO Linear SVM"],
    mlb.classes_
)

pd.DataFrame(conf.most_common(10), columns=["True to Predicted", "Count"])

# Summary & Comparative Model Error Analysis

# Emotion wise performance across models
emotion_model_perf = []

for model_name, report in per_emotion_reports.items():
    for _, row in report.iterrows():
        emotion_model_perf.append({
            "Model": model_name,
            "Emotion": row["Emotion"],
            "F1": row["F1"]
        })

emotion_model_perf_df = pd.DataFrame(emotion_model_perf)

# Best model per emotion
best_model_per_emotion = emotion_model_perf_df.loc[
    emotion_model_perf_df.groupby("Emotion")["F1"].idxmax()
].sort_values("Emotion")

best_model_per_emotion

# Emotion difficulty ranking
emotion_difficulty = (
    emotion_model_perf_df
    .groupby("Emotion")["F1"]
    .mean()
    .sort_values()
    .reset_index()
    .rename(columns={"F1": "Avgerage F1 Across Models"})
)

emotion_difficulty

# Model level error summary
summary_rows = []

for name, preds in all_models.items():
    total_errors = np.sum(y_test != preds)
    total_labels = y_test.size

    summary_rows.append({
        "Model": name,
        "F1-Micro": f1_score(y_test, preds, average="micro"),
        "F1-Macro": f1_score(y_test, preds, average="macro"),
        "Total Errors": total_errors,
        "Error Rate": total_errors / total_labels
    })

summary_error_df = pd.DataFrame(summary_rows)\
    .sort_values("F1-Micro", ascending=False)

summary_error_df